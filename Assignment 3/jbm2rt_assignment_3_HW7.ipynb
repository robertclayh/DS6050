{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: The Dawn of Neural Machine Translation with Seq2Seq\n",
    "\n",
    "## Robert Clay Harris: jbm2rt\n",
    "\n",
    "## Part 1: Historical Context and Motivation\n",
    "\n",
    "Before the rise of deep learning, machine translation (MT) was dominated by **Statistical Machine Translation (SMT)**. SMT systems were complex engineering feats, relying on statistical models to translate phrases piece-by-piece and then reassembling them using intricate rules.\n",
    "\n",
    "In 2014, a seminal paper changed the landscape: **\"Sequence to Sequence Learning with Neural Networks\"** by Sutskever, Vinyals, and Le. They proposed an elegant, end-to-end neural architecture.\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "The core idea is remarkably simple:\n",
    "\n",
    "1. **The Encoder**: An RNN reads the input sentence (e.g., English) one word at a time, compressing the entire meaning into a single, fixed-size vector. This is often called the **context vector** or, more poetically, a **\"thought vector.\"**\n",
    "\n",
    "2. **The Decoder**: Another RNN takes this \"thought vector\" as its starting point and generates the output sentence (e.g., French) one word at a time.\n",
    "\n",
    "This architecture marked the beginning of **Neural Machine Translation (NMT)**. In 2016, Google Translate switched from its older SMT system to NMT. The improvement was dramatic.\n",
    "\n",
    "> **\"With this update, Google Translate is improving more in a single leap than we've seen in the last ten years combined.\"** – [Google Blog, 2016 ](https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/)\n",
    "\n",
    "The original 2014 paper used LSTMs. However, we will use **Gated Recurrent Units (GRUs)** for this assignment just to mix it up! GRUs are similar to LSTMs in that they use gates to control information flow, but their architecture is simpler (two gates vs. three, and no separate cell state). They often perform similarly to LSTMs but are slightly faster to train and easier to implement.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Key Concepts\n",
    "\n",
    "### 2.1 Backpropagation Through Time (BPTT)\n",
    "\n",
    "When training RNNs, we must backpropagate gradients through all time steps of the sequence. This is called **Backpropagation Through Time (BPTT)**. The gradients flow backwards through the unrolled RNN, allowing the model to learn long-term dependencies.\n",
    "\n",
    "### 2.2 BPTT and Truncated BPTT (TBPTT)\n",
    "\n",
    "If a sequence is very long (e.g., modeling an entire document), full BPTT consumes excessive memory because we must store the activations for every time step.\n",
    "\n",
    "**Truncated BPTT (TBPTT)** solves this by breaking the sequence into chunks. We process a chunk, backpropagate gradients only within that chunk, and then pass the hidden state forward to the next chunk, stopping the gradient flow at the chunk boundary.\n",
    "\n",
    "In this assignment, our sentences are short, so we will use standard BPTT.\n",
    "\n",
    "### 2.3 The \"Reversal Trick\"\n",
    "\n",
    "The 2014 paper discovered a surprising trick that significantly boosted performance: **Reverse the source sentence.**\n",
    "\n",
    "- **Original**: [I, love, AI] → [J'aime, l'IA]\n",
    "- **Reversed**: [AI, love, I] → [J'aime, l'IA]\n",
    "\n",
    "By doing this, the first words of the output (J'aime) are very close to the corresponding words in the reversed input (I). This creates short-term dependencies, making it much easier for the optimizer to \"establish communication\" between the input and the output early in training.\n",
    "\n",
    "### 2.4 Teacher Forcing\n",
    "\n",
    "When training the decoder, if we use the model's prediction as the input for the subsequent step, an early mistake can cascade, making training unstable.\n",
    "\n",
    "**Teacher Forcing** is a strategy where we sometimes use the actual ground truth token from the training data as the input for the next step, rather than the model's own prediction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Setup and Data Preprocessing\n",
    "\n",
    "We will use a dataset of English-French sentence pairs.\n",
    "\n",
    "### 3.0 Download the Data\n",
    "\n",
    "Run this cell in Colab to download and unzip the data:\n",
    "\n",
    "```bash\n",
    "!wget https://download.pytorch.org/tutorial/data.zip\n",
    "!unzip -o data.zip\n",
    "```\n",
    "\n",
    "### 3.1 Imports and Utilities (Provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# Utilities for handling variable length sequences\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Define special tokens\n",
    "PAD_IDX = 0\n",
    "SOS_IDX = 1\n",
    "EOS_IDX = 2\n",
    "UNK_IDX = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Vocabulary and Data Loading (Provided)\n",
    "\n",
    "We provide the utilities to load, normalize, and filter the data. We limit the dataset size and sentence length for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Trimmed to 15000 sentence pairs\n",
      "Vocabularies: eng (2830), fra (5098)\n"
     ]
    }
   ],
   "source": [
    "class Lang:\n",
    "    \"\"\"A class to hold the vocabulary of a language.\"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"<PAD>\": PAD_IDX, \"<SOS>\": SOS_IDX, \"<EOS>\": EOS_IDX, \"<UNK>\": UNK_IDX}\n",
    "        self.index2word = {PAD_IDX: \"<PAD>\", SOS_IDX: \"<SOS>\", EOS_IDX: \"<EOS>\", UNK_IDX: \"<UNK>\"}\n",
    "        self.n_words = 4\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = s.lower().strip()\n",
    "    # Normalize Unicode characters (e.g., remove accents)\n",
    "    s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "# We filter for relatively short sentences\n",
    "MAX_LENGTH = 15\n",
    "NUM_EXAMPLES = 15000\n",
    "\n",
    "def prepareData(lang1, lang2):\n",
    "    print(\"Reading lines...\")\n",
    "    lines = open(f'data/{lang1}-{lang2}.txt', encoding='utf-8').read().strip().split('\\n')\n",
    "    \n",
    "    # Limit the number of examples and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines[:NUM_EXAMPLES]]\n",
    "\n",
    "    # Filter pairs by length\n",
    "    pairs = [p for p in pairs if len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH]\n",
    "    \n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "\n",
    "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(f\"Vocabularies: {input_lang.name} ({input_lang.n_words}), {output_lang.name} ({output_lang.n_words})\")\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Dataset and DataLoader (Provided)\n",
    "\n",
    "We implement the PyTorch Dataset. This is where we apply the **Input Reversal Trick**.\n",
    "\n",
    "We also implement a `collate_fn`. This function handles padding sequences in a batch to the same length. Crucially, it also returns the original lengths of the sequences, which we need for **Packing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs, input_lang, output_lang, reverse_source=True):\n",
    "        self.pairs = pairs\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "        self.reverse_source = reverse_source\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def indexesFromSentence(self, lang, sentence):\n",
    "        return [lang.word2index.get(word, UNK_IDX) for word in sentence.split(' ')]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        src_text = pair[0]\n",
    "        tgt_text = pair[1]\n",
    "\n",
    "        src_indices = self.indexesFromSentence(self.input_lang, src_text)\n",
    "        tgt_indices = self.indexesFromSentence(self.output_lang, tgt_text)\n",
    "\n",
    "        # Apply the Reversal Trick to the source sentence\n",
    "        if self.reverse_source:\n",
    "            src_indices.reverse()\n",
    "\n",
    "        # Add EOS token to both\n",
    "        src_indices.append(EOS_IDX)\n",
    "        tgt_indices.append(EOS_IDX)\n",
    "\n",
    "        return torch.tensor(src_indices, dtype=torch.long), \\\n",
    "               torch.tensor(tgt_indices, dtype=torch.long)\n",
    "\n",
    "# Collate function to handle padding and return lengths\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_item, tgt_item in batch:\n",
    "        src_batch.append(src_item)\n",
    "        tgt_batch.append(tgt_item)\n",
    "    \n",
    "    # Get the lengths of the source sequences BEFORE padding\n",
    "    src_lengths = torch.tensor([len(s) for s in src_batch])\n",
    "    \n",
    "    # Pad the sequences\n",
    "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=PAD_IDX)\n",
    "\n",
    "    # We return the lengths as well for packing later\n",
    "    return src_batch.to(device), src_lengths, tgt_batch.to(device)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "BATCH_SIZE = 64\n",
    "dataset = TranslationDataset(pairs, input_lang, output_lang, reverse_source=True)\n",
    "\n",
    "# Split into train and validation (90/10)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: The Seq2Seq Architecture (Implementation Tasks)\n",
    "\n",
    "Now we implement the core components.\n",
    "\n",
    "### Task 1: The Encoder (20 Points)\n",
    "\n",
    "The Encoder processes the input sequence and compresses it into the context vector.\n",
    "\n",
    "**Important: Packing Padded Sequences.** When training RNNs on batches, we must use `pack_padded_sequence`. This tells the GRU/LSTM to ignore PAD tokens. If we don't pack, the RNN processes the padding, which wastes computation and can negatively affect the final hidden state (the context vector).\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Initialize the `nn.Embedding` and `nn.GRU` layers. Use `batch_first=True`.\n",
    "2. In the forward pass, embed the input.\n",
    "3. Pack the embedded sequence using `pack_padded_sequence`.\n",
    "4. Pass the packed sequence through the GRU.\n",
    "5. Return the final hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderGRU(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Embedding\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=PAD_IDX)\n",
    "\n",
    "        # GRU\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hid_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_lengths):\n",
    "        # src: (batch, src_len); src_lengths: (batch,)\n",
    "        embedded = self.dropout(self.embedding(src))  # (batch, src_len, emb_dim)\n",
    "\n",
    "        # Pack (ignore padding)\n",
    "        packed_embedded = pack_padded_sequence(\n",
    "            embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # RNN\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)  # hidden: (n_layers, batch, hid_dim)\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: The Decoder (20 Points)\n",
    "\n",
    "The Decoder takes the context vector as its initial hidden state and generates the output sequence one token at a time.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Initialize the Embedding, GRU, and output Linear (`fc_out`) layers.\n",
    "2. The forward pass accepts one token (`input`) and the previous hidden state.\n",
    "3. Embed the input token (remembering to add a sequence dimension).\n",
    "4. Pass the embedding and hidden state to the GRU.\n",
    "5. Pass the GRU output through the linear layer to get the prediction logits.\n",
    "6. Return the prediction and the new hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderGRU(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Embedding\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=PAD_IDX)\n",
    "\n",
    "        # GRU (must match encoder hid_dim)\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hid_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        # Output projection\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # input: (batch,) -> (batch, 1)\n",
    "        input = input.unsqueeze(1)\n",
    "\n",
    "        # Embed\n",
    "        embedded = self.dropout(self.embedding(input))  # (batch, 1, emb_dim)\n",
    "\n",
    "        # RNN step\n",
    "        output, hidden = self.rnn(embedded, hidden)  # output: (batch, 1, hid_dim)\n",
    "\n",
    "        # Predict logits\n",
    "        prediction = self.fc_out(output.squeeze(1))  # (batch, output_dim)\n",
    "\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: The Seq2Seq Wrapper (30 Points)\n",
    "\n",
    "This class combines the Encoder and Decoder and manages the overall process, including the decoding loop and Teacher Forcing.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Run the encoder on the source sequence and lengths to get the context vector (hidden).\n",
    "2. Initialize the decoder input with the `<SOS>` token.\n",
    "3. Iterate over the length of the target sequence:\n",
    "    - Run the decoder one step.\n",
    "    - Store the output.\n",
    "    - Decide whether to use teacher forcing or the model's own prediction as the next input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, src_lengths, tgt, teacher_forcing_ratio=0.5):\n",
    "        # src: (batch, src_len) | tgt: (batch, tgt_len)\n",
    "        batch_size = src.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size, device=self.device)\n",
    "\n",
    "        # 1) Encode\n",
    "        hidden = self.encoder(src, src_lengths)  # (n_layers, batch, hid_dim)\n",
    "\n",
    "        # 2) First decoder input is <SOS>\n",
    "        input = torch.full((batch_size,), SOS_IDX, dtype=torch.long, device=self.device)\n",
    "\n",
    "        # 3) Decode step-by-step\n",
    "        for t in range(tgt_len):\n",
    "            output, hidden = self.decoder(input, hidden)  # output: (batch, vocab)\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            # 6) Next input\n",
    "            input = tgt[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Training the Model\n",
    "\n",
    "### 5.1 Initialization (Provided)\n",
    "\n",
    "We initialize the model with sensible hyperparameters. We use a relatively small model (2 layers, 512 hidden units) which provides a good balance of capacity and training speed for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 10,162,154 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "INPUT_DIM = input_lang.n_words\n",
    "OUTPUT_DIM = output_lang.n_words\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2 # Using 2 layers\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "# Initialize models (Ensure Tasks 1-3 are completed first!)\n",
    "enc = EncoderGRU(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
    "dec = DecoderGRU(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT).to(device)\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "# Initialize weights (common practice for RNNs)\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Loss function: CrossEntropyLoss, ignoring the padding index\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: The Training and Evaluation Loops (20 Points)\n",
    "\n",
    "Implement the training and evaluation functions.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. In `train`, implement the forward pass, loss calculation, backpropagation (BPTT), gradient clipping, and optimizer step.\n",
    "2. In `evaluate`, implement the forward pass (with `teacher_forcing_ratio=0`).\n",
    "3. **Crucial:** Reshape the output and tgt tensors correctly for the loss function. CrossEntropyLoss expects predictions of shape `(N, C)` and targets of shape `(N)`, where N is the total number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src, src_lengths, tgt = batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1) Forward (default teacher forcing)\n",
    "        output = model(src, src_lengths, tgt)\n",
    "\n",
    "        # 2) Flatten for CE loss\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.reshape(-1, output_dim)\n",
    "        tgt = tgt.reshape(-1)\n",
    "\n",
    "        # 3) Loss\n",
    "        loss = criterion(output, tgt)\n",
    "\n",
    "        # 4) Backprop\n",
    "        loss.backward()\n",
    "\n",
    "        # 5) Clip\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        # 6) Step\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src, src_lengths, tgt = batch\n",
    "\n",
    "            # 1) Forward (no teacher forcing)\n",
    "            output = model(src, src_lengths, tgt, teacher_forcing_ratio=0.0)\n",
    "\n",
    "            # 2) Flatten\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.reshape(-1, output_dim)\n",
    "            tgt = tgt.reshape(-1)\n",
    "\n",
    "            # 3) Loss\n",
    "            loss = criterion(output, tgt)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Running the Training (Provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch: 01 | Time: 4s\n",
      "\tTrain Loss: 4.912 | Train PPL: 135.850\n",
      "\t Val. Loss: 4.508 |  Val. PPL:  90.725\n",
      "Epoch: 02 | Time: 2s\n",
      "\tTrain Loss: 4.059 | Train PPL:  57.940\n",
      "\t Val. Loss: 4.022 |  Val. PPL:  55.810\n",
      "Epoch: 03 | Time: 2s\n",
      "\tTrain Loss: 3.505 | Train PPL:  33.274\n",
      "\t Val. Loss: 3.683 |  Val. PPL:  39.754\n",
      "Epoch: 04 | Time: 2s\n",
      "\tTrain Loss: 3.103 | Train PPL:  22.275\n",
      "\t Val. Loss: 3.466 |  Val. PPL:  32.021\n",
      "Epoch: 05 | Time: 2s\n",
      "\tTrain Loss: 2.768 | Train PPL:  15.919\n",
      "\t Val. Loss: 3.260 |  Val. PPL:  26.040\n",
      "Epoch: 06 | Time: 2s\n",
      "\tTrain Loss: 2.486 | Train PPL:  12.011\n",
      "\t Val. Loss: 3.112 |  Val. PPL:  22.458\n",
      "Epoch: 07 | Time: 2s\n",
      "\tTrain Loss: 2.252 | Train PPL:   9.507\n",
      "\t Val. Loss: 2.992 |  Val. PPL:  19.930\n",
      "Epoch: 08 | Time: 2s\n",
      "\tTrain Loss: 2.057 | Train PPL:   7.824\n",
      "\t Val. Loss: 2.936 |  Val. PPL:  18.831\n",
      "Epoch: 09 | Time: 2s\n",
      "\tTrain Loss: 1.895 | Train PPL:   6.649\n",
      "\t Val. Loss: 2.881 |  Val. PPL:  17.825\n",
      "Epoch: 10 | Time: 2s\n",
      "\tTrain Loss: 1.746 | Train PPL:   5.734\n",
      "\t Val. Loss: 2.848 |  Val. PPL:  17.247\n",
      "Epoch: 11 | Time: 2s\n",
      "\tTrain Loss: 1.618 | Train PPL:   5.044\n",
      "\t Val. Loss: 2.855 |  Val. PPL:  17.367\n",
      "Epoch: 12 | Time: 2s\n",
      "\tTrain Loss: 1.509 | Train PPL:   4.522\n",
      "\t Val. Loss: 2.796 |  Val. PPL:  16.378\n",
      "Epoch: 13 | Time: 2s\n",
      "\tTrain Loss: 1.415 | Train PPL:   4.115\n",
      "\t Val. Loss: 2.795 |  Val. PPL:  16.367\n",
      "Epoch: 14 | Time: 2s\n",
      "\tTrain Loss: 1.320 | Train PPL:   3.745\n",
      "\t Val. Loss: 2.770 |  Val. PPL:  15.952\n",
      "Epoch: 15 | Time: 2s\n",
      "\tTrain Loss: 1.248 | Train PPL:   3.482\n",
      "\t Val. Loss: 2.776 |  Val. PPL:  16.048\n",
      "Epoch: 16 | Time: 2s\n",
      "\tTrain Loss: 1.180 | Train PPL:   3.255\n",
      "\t Val. Loss: 2.793 |  Val. PPL:  16.330\n",
      "Epoch: 17 | Time: 2s\n",
      "\tTrain Loss: 1.123 | Train PPL:   3.074\n",
      "\t Val. Loss: 2.790 |  Val. PPL:  16.277\n",
      "Epoch: 18 | Time: 2s\n",
      "\tTrain Loss: 1.065 | Train PPL:   2.900\n",
      "\t Val. Loss: 2.793 |  Val. PPL:  16.333\n",
      "Epoch: 19 | Time: 2s\n",
      "\tTrain Loss: 1.031 | Train PPL:   2.803\n",
      "\t Val. Loss: 2.795 |  Val. PPL:  16.361\n",
      "Epoch: 20 | Time: 2s\n",
      "\tTrain Loss: 0.984 | Train PPL:   2.676\n",
      "\t Val. Loss: 2.764 |  Val. PPL:  15.869\n",
      "Epoch: 21 | Time: 2s\n",
      "\tTrain Loss: 0.959 | Train PPL:   2.610\n",
      "\t Val. Loss: 2.820 |  Val. PPL:  16.771\n",
      "Epoch: 22 | Time: 2s\n",
      "\tTrain Loss: 0.909 | Train PPL:   2.483\n",
      "\t Val. Loss: 2.841 |  Val. PPL:  17.137\n",
      "Epoch: 23 | Time: 2s\n",
      "\tTrain Loss: 0.891 | Train PPL:   2.437\n",
      "\t Val. Loss: 2.830 |  Val. PPL:  16.942\n",
      "Epoch: 24 | Time: 2s\n",
      "\tTrain Loss: 0.834 | Train PPL:   2.302\n",
      "\t Val. Loss: 2.833 |  Val. PPL:  16.993\n",
      "Epoch: 25 | Time: 2s\n",
      "\tTrain Loss: 0.819 | Train PPL:   2.269\n",
      "\t Val. Loss: 2.895 |  Val. PPL:  18.079\n",
      "Epoch: 26 | Time: 2s\n",
      "\tTrain Loss: 0.794 | Train PPL:   2.212\n",
      "\t Val. Loss: 2.882 |  Val. PPL:  17.847\n",
      "Epoch: 27 | Time: 2s\n",
      "\tTrain Loss: 0.776 | Train PPL:   2.173\n",
      "\t Val. Loss: 2.908 |  Val. PPL:  18.325\n",
      "Epoch: 28 | Time: 2s\n",
      "\tTrain Loss: 0.758 | Train PPL:   2.134\n",
      "\t Val. Loss: 2.926 |  Val. PPL:  18.644\n",
      "Epoch: 29 | Time: 2s\n",
      "\tTrain Loss: 0.729 | Train PPL:   2.072\n",
      "\t Val. Loss: 2.948 |  Val. PPL:  19.065\n",
      "Epoch: 30 | Time: 2s\n",
      "\tTrain Loss: 0.733 | Train PPL:   2.080\n",
      "\t Val. Loss: 2.945 |  Val. PPL:  19.016\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 30\n",
    "CLIP = 1.0\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_secs = int(end_time - start_time)\n",
    "\n",
    "    # Save best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'seq2seq-gru-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note on Training Duration:**\n",
    "\n",
    "Vanilla Seq2Seq models typically require 30-50+ epochs to produce reasonable translations. If you train for only 15 epochs, you will likely see:\n",
    "- Decreasing loss (showing the model is learning)\n",
    "- But poor actual translations (the model hasn't converged yet)\n",
    "\n",
    "This is normal! The model needs more time to learn the complex mapping between languages.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 6: Inference and Analysis (10 Points)\n",
    "\n",
    "### 6.1 Inference (Greedy Decoding)\n",
    "\n",
    "During inference, we don't have the target sentence, so teacher forcing is impossible. We use the model's own predictions at each step. The simplest method is **Greedy Decoding**: always choose the word with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN: i am cold\n",
      "FR: je je ?\n",
      "\n",
      "EN: she is happy\n",
      "FR: elle est en .\n",
      "\n",
      "EN: he is running\n",
      "FR: il est en .\n",
      "\n",
      "EN: we are ready\n",
      "FR: nous sommes en .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def translate_sentence(sentence, src_lang, tgt_lang, model, device, max_len=50):\n",
    "    model.eval()\n",
    "\n",
    "    # 1. Preprocess the input sentence (normalize and reverse!)\n",
    "    normalized_sentence = normalizeString(sentence)\n",
    "    reversed_sentence = ' '.join(normalized_sentence.split(' ')[::-1])\n",
    "\n",
    "    # 2. Convert to indices and tensor\n",
    "    indices = [src_lang.word2index.get(word, UNK_IDX) for word in reversed_sentence.split(' ')] + [EOS_IDX]\n",
    "    src_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device) # (1, T)\n",
    "    src_len = torch.tensor([len(indices)])\n",
    "\n",
    "    # 3. Encode the sentence\n",
    "    with torch.no_grad():\n",
    "        hidden = model.encoder(src_tensor, src_len)\n",
    "\n",
    "    # 4. Start decoding\n",
    "    trg_indices = [SOS_IDX]\n",
    "    input_tensor = torch.tensor([SOS_IDX], dtype=torch.long).to(device) # (1)\n",
    "\n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model.decoder(input_tensor, hidden)\n",
    "\n",
    "        # 5. Greedy Decoding\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indices.append(pred_token)\n",
    "\n",
    "        # Check for <EOS>\n",
    "        if pred_token == EOS_IDX:\n",
    "            break\n",
    "\n",
    "        # Prepare the next input\n",
    "        input_tensor = torch.tensor([pred_token], dtype=torch.long).to(device)\n",
    "\n",
    "    # 6. Convert indices back to words\n",
    "    trg_tokens = [tgt_lang.index2word[i] for i in trg_indices]\n",
    "    return trg_tokens[1:-1] # Exclude <SOS> and <EOS>\n",
    "\n",
    "# Qualitative Analysis (Uncomment after training)\n",
    "model.load_state_dict(torch.load('seq2seq-gru-model.pt'))\n",
    "examples = [\"i am cold\", \"she is happy\", \"he is running\", \"we are ready\"]\n",
    "for example in examples:\n",
    "    translation = translate_sentence(example, input_lang, output_lang, model, device)\n",
    "    print(f\"EN: {example}\")\n",
    "    print(f\"FR: {' '.join(translation)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Understanding Your Results\n",
    "\n",
    "**Expected Performance:**\n",
    "\n",
    "After training, you may notice that your translations are not perfect - and that's completely normal! Here's what you should expect:\n",
    "\n",
    "**What Good Results Look Like:**\n",
    "- Training loss decreasing from ~5.0 to ~1.0-1.5\n",
    "- Validation loss around 2.5-3.5\n",
    "- Some simple phrases translating correctly (e.g., \"how are you\" → \"comment vas tu\")\n",
    "- Shorter sentences working better than longer ones\n",
    "\n",
    "**Why Translations May Be Poor:**\n",
    "\n",
    "1. **The Information Bottleneck**: This is the fundamental limitation we've been discussing. The entire English sentence must be compressed into a single fixed-size vector (512 numbers). For complex sentences, critical information gets lost.\n",
    "\n",
    "2. **Insufficient Training**: 30 epochs on this small dataset is barely enough. Production NMT systems train for much longer on millions of examples.\n",
    "\n",
    "3. **Overfitting**: If your validation loss is significantly higher than training loss (e.g., 2.8 vs 1.3), the model is memorizing training patterns rather than learning to translate.\n",
    "\n",
    "4. **Common Phrase Bias**: The model often outputs frequent French phrases (like \"je suis...\") regardless of the actual input, because these patterns were common in training data.\n",
    "\n",
    "5. **Greedy Decoding**: We always pick the highest probability word. Beam search (which considers multiple possibilities) would improve results.\n",
    "\n",
    "**What Your Model Is Actually Learning:**\n",
    "\n",
    "Look at a translation like:\n",
    "```\n",
    "\"i am cold\" → \"je suis serieux\"\n",
    "```\n",
    "\n",
    "The model correctly learned:\n",
    "- \"i am\" → \"je suis\" ✓\n",
    "- But outputs a common word \"serieux\" instead of \"froid\"\n",
    "\n",
    "This shows the model IS learning French grammar and common patterns, just not the specific vocabulary mapping yet.\n",
    "\n",
    "**This Is Why Attention Was Invented!**\n",
    "\n",
    "The poor performance of vanilla Seq2Seq on longer sentences directly motivated the invention of attention mechanisms (covered in the next module). Attention allows the decoder to \"look back\" at different parts of the input instead of relying on a single compressed vector.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Bonus: Diagnostic Function (Optional)\n",
    "\n",
    "To better understand what your model has learned, implement this diagnostic function that checks if the model can at least memorize some training examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL DIAGNOSIS - Testing on Training Examples\n",
      "======================================================================\n",
      "\n",
      "Example 1:\n",
      "  EN (input):     go .\n",
      "  FR (expected):  va !\n",
      "  FR (predicted): va !\n",
      "  Word overlap:   2/2 (100.0%)\n",
      "\n",
      "Example 2:\n",
      "  EN (input):     run !\n",
      "  FR (expected):  cours !\n",
      "  FR (predicted): courez !\n",
      "  Word overlap:   1/2 (50.0%)\n",
      "\n",
      "Example 3:\n",
      "  EN (input):     run !\n",
      "  FR (expected):  courez !\n",
      "  FR (predicted): courez !\n",
      "  Word overlap:   2/2 (100.0%)\n",
      "\n",
      "Example 4:\n",
      "  EN (input):     wow !\n",
      "  FR (expected):  ca alors !\n",
      "  FR (predicted): ca alors !\n",
      "  Word overlap:   3/3 (100.0%)\n",
      "\n",
      "Example 5:\n",
      "  EN (input):     fire !\n",
      "  FR (expected):  au feu !\n",
      "  FR (predicted): au feu !\n",
      "  Word overlap:   3/3 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "def diagnose_model(model, src_lang, tgt_lang, pairs, device, num_examples=5):\n",
    "    \"\"\"\n",
    "    Check if model can translate training examples (memorization test)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"MODEL DIAGNOSIS - Testing on Training Examples\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        en_sentence = pairs[i][0]\n",
    "        fr_actual = pairs[i][1]\n",
    "        fr_predicted = translate_sentence(en_sentence, src_lang, tgt_lang, model, device)\n",
    "        \n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"  EN (input):     {en_sentence}\")\n",
    "        print(f\"  FR (expected):  {fr_actual}\")\n",
    "        print(f\"  FR (predicted): {' '.join(fr_predicted)}\")\n",
    "        \n",
    "        # Calculate word overlap\n",
    "        expected_words = set(fr_actual.split())\n",
    "        predicted_words = set(fr_predicted)\n",
    "        overlap = expected_words.intersection(predicted_words)\n",
    "        if len(expected_words) > 0:\n",
    "            accuracy = len(overlap) / len(expected_words) * 100\n",
    "            print(f\"  Word overlap:   {len(overlap)}/{len(expected_words)} ({accuracy:.1f}%)\")\n",
    "\n",
    "# Run after loading best model\n",
    "diagnose_model(model, input_lang, output_lang, pairs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8kEIsDB-PlS"
   },
   "source": [
    "If the model can't even memorize training examples with >50% word overlap, it needs more training epochs or there may be a bug.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Conceptual Questions\n",
    "\n",
    "Answer the following questions in a separate text cell or document:\n",
    "\n",
    "1. **The Information Bottleneck**: The core limitation of this architecture is that the encoder must compress the entire input sentence into a single fixed-size context vector (hidden). Why is this a significant problem when translating very long or complex sentences?\n",
    "\n",
    "This architecture forces the encoder to compress all information about the input sentence—its syntax, semantics, and context—into a single fixed-length vector. For short sentences, this can be adequate, but for long or complex ones, important details are lost because the hidden state cannot represent all dependencies effectively. As a result, the decoder lacks access to earlier parts of the input, leading to incomplete or incorrect translations.\n",
    "\n",
    "2. **Input Reversal**: Explain again, in your own words, why reversing the input (the \"Reversal Trick\") helped the model learn more effectively. Relate your answer to the concept of gradient flow in BPTT.\n",
    "\n",
    "Reversing the input shortens the effective distance between corresponding words in the source and target sequences. During Backpropagation Through Time (BPTT), gradients must flow through fewer time steps to connect related words, reducing vanishing gradient effects. This helps the model learn word alignments more easily and speeds up convergence in training.\n",
    "\n",
    "3. **TBPTT Application**: While we used standard BPTT here, describe a different NLP task where Truncated BPTT (TBPTT) would be essential, and explain why standard BPTT would be unsuitable in that scenario.\n",
    "\n",
    "Truncated BPTT would be essential in tasks such as language modeling on long documents or streaming text, where sequences can have thousands of tokens. Standard BPTT would require storing all intermediate activations across the full sequence, which is computationally infeasible. TBPTT limits backpropagation to manageable chunks, allowing the model to train efficiently while still maintaining continuity through hidden states.\n",
    "\n",
    "4. **Packing**: Why is it important to use `pack_padded_sequence` in the encoder when dealing with batched inputs? What might happen if we didn't use it?\n",
    "\n",
    "`pack_padded_sequence` ensures the RNN ignores padded elements when processing batched sequences of different lengths. Without packing, the model would treat padding tokens as real inputs, causing gradients and hidden states to be influenced by meaningless padding values. This would waste computation and degrade both training efficiency and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRz38vQV1kVw"
   },
   "source": [
    "For those that are interested to improve the performance, try to add:[optional]\n",
    "- Beam Search for better decoding (instead of greedy)\n",
    "- Better evaluation metrics (BLEU score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] EN: be serious .\n",
      "     REF: soyez serieuses !\n",
      "     PRD: sois serieuse !\n",
      "     BLEU: 0.114\n",
      "\n",
      "[02] EN: how thrilling !\n",
      "     REF: comme c est excitant !\n",
      "     PRD: comme c est romantique !\n",
      "     BLEU: 0.286\n",
      "\n",
      "[03] EN: what is it ?\n",
      "     REF: qu est ce ?\n",
      "     PRD: qu est ce que c est ?\n",
      "     BLEU: 0.176\n",
      "\n",
      "[04] EN: i was unprepared .\n",
      "     REF: je n etais pas prepare .\n",
      "     PRD: je n ai pas arme .\n",
      "     BLEU: 0.103\n",
      "\n",
      "[05] EN: i have no idea .\n",
      "     REF: aucune idee .\n",
      "     PRD: je n ai aucune idee .\n",
      "     BLEU: 0.202\n",
      "\n",
      "[06] EN: here he comes .\n",
      "     REF: le voila .\n",
      "     PRD: le voila .\n",
      "     BLEU: 0.562\n",
      "\n",
      "[07] EN: i feel weak .\n",
      "     REF: je me sens faible .\n",
      "     PRD: je me sens faible .\n",
      "     BLEU: 1.000\n",
      "\n",
      "[08] EN: this feels right .\n",
      "     REF: ca semble correct .\n",
      "     PRD: ca semble correct .\n",
      "     BLEU: 1.000\n",
      "\n",
      "[09] EN: are you from here ?\n",
      "     REF: vous etes du coin ?\n",
      "     PRD: vous habitez ici ?\n",
      "     BLEU: 0.074\n",
      "\n",
      "[10] EN: who painted that ?\n",
      "     REF: qui a peint cela ?\n",
      "     PRD: qui a peint ceci ?\n",
      "     BLEU: 0.286\n",
      "\n",
      "Average BLEU over 10 examples: 0.380\n"
     ]
    }
   ],
   "source": [
    "# Optional Extension: Beam Search + BLEU Evaluation\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "smooth = SmoothingFunction().method1\n",
    "\n",
    "def translate_beam(sentence, src_lang, tgt_lang, model, device, max_len=50, beam_size=5):\n",
    "    \"\"\"Simple beam search (no length norm).\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # preprocess (normalize + reverse)\n",
    "    norm = normalizeString(sentence)\n",
    "    rev = ' '.join(norm.split(' ')[::-1])\n",
    "    idxs = [src_lang.word2index.get(w, UNK_IDX) for w in rev.split(' ')] + [EOS_IDX]\n",
    "    src = torch.tensor(idxs, dtype=torch.long).unsqueeze(0).to(device)  # (1, T)\n",
    "    src_len = torch.tensor([len(idxs)])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden = model.encoder(src, src_len)\n",
    "\n",
    "    # beams: list of (token_seq_tensor, score, hidden)\n",
    "    beams = [(torch.tensor([SOS_IDX], device=device), 0.0, hidden)]\n",
    "    finished = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        new_beams = []\n",
    "        for seq, score, h in beams:\n",
    "            inp = seq[-1].unsqueeze(0)  # (1,)\n",
    "            with torch.no_grad():\n",
    "                out, h2 = model.decoder(inp, h)  # out: (1, vocab)\n",
    "            logp = F.log_softmax(out, dim=1).squeeze(0)  # (vocab,)\n",
    "            topk = torch.topk(logp, beam_size)\n",
    "            for tok, tok_lp in zip(topk.indices, topk.values):\n",
    "                tok = tok.item()\n",
    "                new_seq = torch.cat([seq, torch.tensor([tok], device=device)])\n",
    "                new_score = score + tok_lp.item()\n",
    "                if tok == EOS_IDX:\n",
    "                    finished.append((new_seq, new_score))\n",
    "                else:\n",
    "                    new_beams.append((new_seq, new_score, h2))\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "        if not beams:\n",
    "            break\n",
    "\n",
    "    if not finished:\n",
    "        finished = beams\n",
    "    best_seq = max(finished, key=lambda x: x[1])[0].tolist()\n",
    "\n",
    "    # map tokens to words, drop SOS/EOS/PAD\n",
    "    toks = [tgt_lang.index2word[i] for i in best_seq[1:-1] if i not in (SOS_IDX, EOS_IDX, PAD_IDX)]\n",
    "    return toks\n",
    "\n",
    "def bleu_on_examples(model, pairs, src_lang, tgt_lang, device, n=20, beam_size=5, seed=123):\n",
    "    \"\"\"Compute sentence-level BLEU on n random pairs using beam decoding.\"\"\"\n",
    "    random.seed(seed)\n",
    "    subset = random.sample(pairs, min(n, len(pairs)))\n",
    "    scores = []\n",
    "\n",
    "    for i, (en_sentence, fr_ref) in enumerate(subset, 1):\n",
    "        pred_tokens = translate_beam(en_sentence, src_lang, tgt_lang, model, device, beam_size=beam_size)\n",
    "        ref_tokens = fr_ref.split()\n",
    "        bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smooth)\n",
    "        scores.append(bleu)\n",
    "        print(f\"[{i:02d}] EN: {en_sentence}\")\n",
    "        print(f\"     REF: {' '.join(ref_tokens)}\")\n",
    "        print(f\"     PRD: {' '.join(pred_tokens)}\")\n",
    "        print(f\"     BLEU: {bleu:.3f}\\n\")\n",
    "\n",
    "    avg_bleu = float(np.mean(scores)) if scores else 0.0\n",
    "    print(f\"Average BLEU over {len(scores)} examples: {avg_bleu:.3f}\")\n",
    "    return scores, avg_bleu\n",
    "\n",
    "# Example usage\n",
    "model.load_state_dict(torch.load('seq2seq-gru-model.pt', map_location=device))\n",
    "_ = bleu_on_examples(model, pairs, input_lang, output_lang, device, n=10, beam_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] EN: be serious .\n",
      "     REF: soyez serieuses !\n",
      "   GREEDY: sois serieuse ! | BLEU: 0.114\n",
      "     BEAM: sois serieuse ! | BLEU: 0.114\n",
      "\n",
      "[02] EN: how thrilling !\n",
      "     REF: comme c est excitant !\n",
      "   GREEDY: comme c est romantique ! | BLEU: 0.286\n",
      "     BEAM: comme c est romantique ! | BLEU: 0.286\n",
      "\n",
      "[03] EN: what is it ?\n",
      "     REF: qu est ce ?\n",
      "   GREEDY: qu est ce que c est ? | BLEU: 0.176\n",
      "     BEAM: qu est ce que c est ? | BLEU: 0.176\n",
      "\n",
      "[04] EN: i was unprepared .\n",
      "     REF: je n etais pas prepare .\n",
      "   GREEDY: je n ai pas arme . | BLEU: 0.103\n",
      "     BEAM: je n ai pas arme . | BLEU: 0.103\n",
      "\n",
      "[05] EN: i have no idea .\n",
      "     REF: aucune idee .\n",
      "   GREEDY: je n ai aucune idee . | BLEU: 0.202\n",
      "     BEAM: je n ai aucune idee . | BLEU: 0.202\n",
      "\n",
      "[06] EN: here he comes .\n",
      "     REF: le voila .\n",
      "   GREEDY: le voila . | BLEU: 0.562\n",
      "     BEAM: le voila . | BLEU: 0.562\n",
      "\n",
      "[07] EN: i feel weak .\n",
      "     REF: je me sens faible .\n",
      "   GREEDY: je me sens faible . | BLEU: 1.000\n",
      "     BEAM: je me sens faible . | BLEU: 1.000\n",
      "\n",
      "[08] EN: this feels right .\n",
      "     REF: ca semble correct .\n",
      "   GREEDY: ca semble correct . | BLEU: 1.000\n",
      "     BEAM: ca semble correct . | BLEU: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def translate_greedy(sentence, src_lang, tgt_lang, model, device, max_len=50):\n",
    "    return translate_sentence(sentence, src_lang, tgt_lang, model, device, max_len=max_len)\n",
    "\n",
    "def compare_decoders(model, pairs, src_lang, tgt_lang, device, n=10, seed=123):\n",
    "    random.seed(seed)\n",
    "    subset = random.sample(pairs, min(n, len(pairs)))\n",
    "    for i, (en_sentence, fr_ref) in enumerate(subset, 1):\n",
    "        pred_g = translate_greedy(en_sentence, src_lang, tgt_lang, model, device)\n",
    "        pred_b = translate_beam(en_sentence, src_lang, tgt_lang, model, device, beam_size=5)\n",
    "        ref = fr_ref.split()\n",
    "        bleu_g = sentence_bleu([ref], pred_g, smoothing_function=smooth)\n",
    "        bleu_b = sentence_bleu([ref], pred_b, smoothing_function=smooth)\n",
    "        print(f\"[{i:02d}] EN: {en_sentence}\")\n",
    "        print(f\"     REF: {' '.join(ref)}\")\n",
    "        print(f\"   GREEDY: {' '.join(pred_g)} | BLEU: {bleu_g:.3f}\")\n",
    "        print(f\"     BEAM: {' '.join(pred_b)} | BLEU: {bleu_b:.3f}\\n\")\n",
    "\n",
    "compare_decoders(model, pairs, input_lang, output_lang, device, n=8)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
